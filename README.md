# Real-time Facial Expression Detection with 3-Model Comparison

This project provides an interactive web interface to compare the performance of three different machine learning models for real-time facial expression recognition. The application can process video from a webcam or a static uploaded image.

![Demo Screenshot](https://i.imgur.com/example-screenshot.png) 
(Note: Replace this with a screenshot or GIF of your running application!)

## Features

-   *Live Webcam Analysis:* Provides real-time predictions from a webcam feed.
-   *Static Image Upload:* Analyze facial expressions in any uploaded image.
-   *Side-by-Side Model Comparison:* Simultaneously view the predictions from three distinct models:
    1.  *LBP + KNN:* A classic texture-based approach using Local Binary Patterns and a K-Nearest Neighbors classifier.
    2.  *HOG + SVM:* A classic shape-based approach using Histogram of Oriented Gradients and a Support Vector Machine.
    3.  *Mini-Xception (CNN):* A deep learning approach using a lightweight Convolutional Neural Network.
-   *Performance Metrics:* Displays the prediction, latency (ms), and confidence score (for the CNN) for each model.
-   *Smooth UI:* Implements frame skipping on the webcam feed to ensure a high FPS user experience, even on modest hardware.

## Model Performance

The LBP+KNN and HOG+SVM models were trained on the FER-2013 training dataset. The following accuracies were achieved on a test run of 50 randomly selected images from the FER-2013 test set.

| Model                   | Accuracy (%) |
| ----------------------- | :----------: |
| LBP + KNN               |    41.20%    |
| HOG + SVM               |    52.40%    |
| *Mini-Xception (CNN)* |  *65.80%*  |

As expected, the deep learning (CNN) model significantly outperforms the classic machine learning approaches.

## Project Structure

.
├── emotion_detector.py         # Main Gradio application script
├── train_models.py             # Script to train KNN and SVM models
├── requirements.txt            # List of Python dependencies
│
├── fer2013/                    # Folder for the dataset
│   ├── train/
│   │   ├── angry/
│   │   └── ...
│   └── test/
│       ├── angry/
│       └── ...
│
├── fer2013_mini_XCEPTION.102-0.66.hdf5  # Pre-trained CNN model weights
├── knn_model.joblib            # Trained KNN model (generated by train_models.py)
└── svm_model.joblib            # Trained SVM model (generated by train_models.py)

## Technologies Used

-   *Python 3*
-   *Gradio:* For building the interactive web UI
-   *OpenCV:* For face detection and image processing
-   *Scikit-learn:* For the KNN and SVM models
-   *Scikit-image:* For LBP and HOG feature extraction
-   *TensorFlow/Keras:* For loading and running the CNN model
-   *Joblib:* For saving and loading trained Scikit-learn models

## Setup and Installation

Follow these steps to set up and run the project locally.

*1. Clone the Repository*
git clone https://github.com/your-username/your-repository-name.git
cd your-repository-name

*2. Create a Virtual Environment (Recommended)*
python -m venv venv
# On Windows
venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate

*3. Install Dependencies*
Create a requirements.txt file with the content below, then run the pip install command.

*requirements.txt*:
gradio
opencv-python
scikit-learn
scikit-image
numpy
tensorflow
joblib
tqdm

Installation command:
pip install -r requirements.txt

*4. Download the Dataset*
-   Download the FER-2013 dataset, for example from [Kaggle](https://www.kaggle.com/datasets/msambare/fer2013).
-   Ensure it is extracted into a folder named fer2013 inside the project directory, with train and test subdirectories.

*5. Download the Pre-trained CNN Model*
-   Download the fer2013_mini_XCEPTION.102-0.66.hdf5 file. A common source is the original author's repository [here](https://github.com/oarriaga/face_classification/blob/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5).
-   Place this .hdf5 file in the root of the project directory.

## Usage

The project requires a one-time training step before the main application can be run.

*Step 1: Train the KNN and SVM Models*
Run the training script from your terminal. This will process the entire training dataset and save the trained models as .joblib files.

*Note:* This process is computationally intensive and may take 20-60 minutes depending on your computer's CPU.

python train_models.py
Upon completion, you will see knn_model.joblib and svm_model.joblib in your project folder.

*Step 2: Run the Main Application*
With the models trained and all files in place, you can now launch the Gradio web interface.

python emotion_detector.py
A local URL (e.g., http://127.0.0.1:7860) will be printed in your terminal. Open this link in your web browser to use the application.
i.imgur.com